{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the download of street network indices\n",
    "\n",
    "As an initial step, the street networks for 176 capital cities were downloaded. A CSV file with the list of cities is provided in the data folder.\n",
    "\n",
    "The download notebook uses Ray as the library for parallel processing. It is adviced to use a Linux machine for running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import ray\n",
    "import modules.network_extractor as net_extractor\n",
    "import geopandas as geopd\n",
    "import pandas as pd\n",
    "from shapely import ops\n",
    "from osmnx import settings\n",
    "import pyproj\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add here the absolute path to the data folder\n",
    "data_base_path = \"\"\n",
    "\n",
    "# The extractor instance\n",
    "extractor = net_extractor.NetworkExtractor(base_path=data_base_path)\n",
    " \n",
    "# Custom OSMnx settings\n",
    "settings.default_crs = \"epsg:4326\"\n",
    "\n",
    "# Initialize Ray\n",
    "# First shut down Ray if it was already initialized.\n",
    "ray.shutdown()\n",
    "# This will initialize Ray with the default settings, which uses all available CPUs.\n",
    "ray.init()\n",
    "\n",
    "# Instead, use the following line for specific resource allocation\n",
    "# ray.init(num_cpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from the GHS dataset\n",
    "urban_centers: geopd.GeoDataFrame\n",
    "\n",
    "ghs_dataset_path = f\"{data_base_path}/GHS_URBAN_CENTERS/GHS_URBAN_SIMPLIFIED_fixed.gpkg\"\n",
    "urban_centers = geopd.read_file(\n",
    "    ghs_dataset_path,\n",
    "    layer='GHS_URBAN_SIMPLIFIED_fixed'    \n",
    ")\n",
    "urban_centers = urban_centers.rename(columns={\n",
    "    \"GC_POP_TOT_2025\": \"population\",\n",
    "    \"GC_UCA_KM2_2025\": \"area\",\n",
    "    \"GC_DEV_USR_2025\": \"continent\",\n",
    "    \"GC_UCN_MAI_2025\": \"name\",\n",
    "    \"GC_CNT_GAD_2025\": \"country\"\n",
    "})\n",
    "transform = pyproj.Transformer.from_crs(\"ESRI:54009\", \"EPSG:4326\", always_xy=True).transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file with the capital cities that are used for the analysis.\n",
    "capital_cities = f\"{data_base_path}/capital_cities.csv\"\n",
    "capital_df = pd.read_csv(capital_cities, delimiter=\",\", header=None)\n",
    "countries = list(capital_df[0])\n",
    "city_names = list(capital_df[1])\n",
    "\n",
    "# Create a DataFrame with the capitals.\n",
    "capitals = pd.DataFrame()\n",
    "for i in range(len(countries)):\n",
    "    capital_city = urban_centers.loc[\n",
    "        (urban_centers[\"name\"] == city_names[i]) &\n",
    "        (urban_centers[\"country\"] == countries[i])\n",
    "    ]\n",
    "    capitals = pd.concat([capitals, capital_city])\n",
    "\n",
    "capitals = capitals.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the names of the capitals to have them all in lowercase and slug_case without special characters.\n",
    "\n",
    "capitals[\"display_name\"] = pd.Series()\n",
    "for cap in capitals.iterrows():\n",
    "    # the city name in lowercase and slug_case for creating the folder to store the graphs and shapefiles\n",
    "    city_name = cap[1][1].replace(\" \", \"_\").lower()\n",
    "\n",
    "    # The name to search the city in the GHS dataset. Capital case. Also used for the DEM.\n",
    "    search_name = cap[1][1]\n",
    "\n",
    "    # The country in which the city is located for searching the GHS dataset. Capital case.\n",
    "    country = cap[1][3]\n",
    "\n",
    "    # extract info from GHS with search_name and country\n",
    "    city_info = urban_centers.loc[\n",
    "        (urban_centers[\"name\"] == search_name) &\n",
    "        (urban_centers[\"country\"] == country)\n",
    "    ]\n",
    "\n",
    "    if len(city_info) == 0:\n",
    "        print(f\"City {search_name}, {country} not found\")\n",
    "\n",
    "    else:\n",
    "        geom = city_info[\"geometry\"].values[0]\n",
    "        geom = ops.transform(transform, geom)\n",
    "\n",
    "    # \n",
    "    o_b = \"{\"\n",
    "    c_b = \"}\"\n",
    "    backslash = f\"\\\\\"\n",
    "    specials = \"áéíóú'șăŏã\"\n",
    "    replaces = \"aeiou_saoa\"\n",
    "\n",
    "    display_name = city_name\n",
    "    modified = False\n",
    "\n",
    "    if \"[\" in display_name or \"]\" in display_name:\n",
    "        modified  = True\n",
    "\n",
    "    for i in range(len(specials)):\n",
    "        if specials[i] in display_name:\n",
    "            modified  = True\n",
    "        display_name = display_name.replace(specials[i], replaces[i])\n",
    "        display_name = display_name.replace(\"[\", \"\")\n",
    "        display_name = display_name.replace(\"]\", \"\")\n",
    "\n",
    "    capitals.loc[cap[0], \"display_name\"] = display_name\n",
    "\n",
    "    # A geometry is built for the download of the DEM from Google Earth Engine.\n",
    "    # Uncomment the next 2 lines for printing the geometry in the format for Google Earth Engine.\n",
    "    # ee_str = f\"{o_b}'geometry': ee.Geometry.BBox{geom.bounds}, 'name': '{display_name}' {c_b},\"\n",
    "    # print(ee_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the street networks for each capital city.\n",
    "# It uses Ray (https://pypi.org/project/ray/) for parallelizing the download of the networks.\n",
    "\n",
    "# Warning: This part requires massive computational resources. For large cities, such as Tokyo,\n",
    "# do not use parallelization, as it may deplete the memory fast.\n",
    "\n",
    "errors = []\n",
    "for cap in capitals.iterrows():\n",
    "    display_name = cap[1][\"display_name\"]\n",
    "    city_name = display_name\n",
    "    # The name to search the city in the GHS dataset. Capital case. Also used for the DEM.\n",
    "    search_name = cap[1][1]\n",
    "    \n",
    "    # The country in which the city is located for searching the GHS dataset. Capital case.\n",
    "    country = cap[1][3]\n",
    "    \n",
    "    print(f\"starting {city_name}, {search_name}, {country}\")\n",
    "\n",
    "    try:\n",
    "        # extract info from GHS with search_name and country\n",
    "        city_info = urban_centers.loc[\n",
    "            (urban_centers[\"name\"] == search_name) &\n",
    "            (urban_centers[\"country\"] == country)\n",
    "        ]\n",
    "\n",
    "        if len(city_info) == 0:\n",
    "            print(f\"City {search_name}, {country} not found\")\n",
    "\n",
    "        else:\n",
    "            geom = city_info[\"geometry\"].values[0]\n",
    "            geom = ops.transform(transform, geom)\n",
    "\n",
    "        print(geom.bounds)\n",
    "\n",
    "        # the geometry from which to extract the network. Is given by the GHS dataset.\n",
    "        geometry = geom\n",
    "\n",
    "        # Variables for the elimination of duplicate pedestrian/driving streets\n",
    "        dist_threshold = 20\n",
    "        slope_threshold = 15\n",
    "\n",
    "        # Assessment = False, so duplicate pedestrian/driving streets will be eliminated.\n",
    "        assess = False\n",
    "\n",
    "        # Create graph and shapefile folders if they do not exist\n",
    "        Path(f\"{data_base_path}/{city_name}/graph\").mkdir(parents=True, exist_ok=True) # graphml folder\n",
    "        Path(f\"{data_base_path}/{city_name}/shp\").mkdir(parents=True, exist_ok=True) # shapefiles folder\n",
    "\n",
    "        # Process the 4 networks in parallel using the paralellized download_network function.\n",
    "        g_promises = []\n",
    "\n",
    "        # the pedestrian network\n",
    "        g_promises.append(extractor.download_network.remote(\n",
    "            extractor,\n",
    "            \"walk\", \n",
    "            geometry, \n",
    "            city_name, \n",
    "            assessment=assess, \n",
    "            dist_threshold=dist_threshold, \n",
    "            slope_threshold=slope_threshold,\n",
    "            add_elevation=True,\n",
    "        ))\n",
    "\n",
    "        # the cycling network\n",
    "        g_promises.append(extractor.download_network.remote(\n",
    "            extractor,\n",
    "            \"bike\", \n",
    "            geometry, \n",
    "            city_name, \n",
    "            assessment=assess, \n",
    "            add_elevation=True,\n",
    "        ))\n",
    "\n",
    "        # # the driving network\n",
    "        g_promises.append(extractor.download_network.remote(\n",
    "            extractor,\n",
    "            \"drive\", \n",
    "            geometry, \n",
    "            city_name, \n",
    "            assessment=assess, \n",
    "            add_elevation=True,\n",
    "        ))\n",
    "\n",
    "        # the public transport network\n",
    "        g_promises.append(extractor.download_network.remote(\n",
    "            extractor,\n",
    "            \"public_transport\", \n",
    "            geometry, \n",
    "            city_name, \n",
    "            assessment=assess, \n",
    "            add_elevation=False,\n",
    "        ))\n",
    "\n",
    "        [g_walk, g_bike, g_drive, g_public] = ray.get(g_promises)\n",
    "\n",
    "        # Save graphs\n",
    "        extractor.save_as_graph(g_walk, f'{city_name}/graph/walk_{city_name}')\n",
    "        extractor.save_as_graph(g_bike, f'{city_name}/graph/bike_{city_name}')\n",
    "        extractor.save_as_graph(g_drive, f'{city_name}/graph/drive_{city_name}')\n",
    "        extractor.save_as_graph(g_public, f'{city_name}/graph/public_{city_name}')\n",
    "\n",
    "        # Save shapefiles\n",
    "        extractor.save_as_shp(g_walk, f'{city_name}/shp/walk_{city_name}')    \n",
    "        extractor.save_as_shp(g_bike, f'{city_name}/shp/bike_{city_name}')\n",
    "        extractor.save_as_shp(g_drive, f'{city_name}/shp/drive_{city_name}')\n",
    "        has_edges = g_public.number_of_edges() > 0 # save only if edges exist\n",
    "        has_nodes = g_public.number_of_nodes() > 0 # save only if nodes exist\n",
    "        extractor.save_as_shp(g_public, f'{city_name}/shp/public_{city_name}', save_edges=has_edges, save_nodes=has_nodes)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        errors.append(city_name)\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the errors in case there were any\n",
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
