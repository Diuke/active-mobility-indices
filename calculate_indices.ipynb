{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the calculation of street network indices.\n",
    "\n",
    "This notebook contains all the code that was used for calculating the street network indices of all 176 capital cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 22:46:53,177\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import ray\n",
    "import modules.network_extractor as net_extractor\n",
    "import geopandas as geopd\n",
    "import pandas as pd\n",
    "from shapely import ops\n",
    "from osmnx import settings\n",
    "import pyproj\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add here the absolute path to the data folder\n",
    "data_base_path = \"\"\n",
    "\n",
    "# The extractor instance\n",
    "extractor = net_extractor.NetworkExtractor(base_path=data_base_path)\n",
    " \n",
    "# Custom OSMnx settings\n",
    "settings.default_crs = \"epsg:4326\"\n",
    "\n",
    "# Initialize Ray\n",
    "# Ray is required as the indices of proximity to POIs and to public transport are parallelized. \n",
    "# First shut down Ray if it was already initialized.\n",
    "ray.shutdown()\n",
    "# This will initialize Ray with the default settings, which uses all available CPUs.\n",
    "ray.init()\n",
    "\n",
    "# Instead, use the following line for specific resource allocation\n",
    "# ray.init(num_cpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from the GHS dataset\n",
    "urban_centers: geopd.GeoDataFrame\n",
    "\n",
    "ghs_geom_path = f\"{data_base_path}/GHS_URBAN_CENTERS/GHS_URBAN_SIMPLIFIED_fixed.gpkg\"\n",
    "\n",
    "ghs_dataset_path = f\"{data_base_path}/GHS_URBAN_CENTERS/GHS_UCDB_GLOBE_R2024A.gpkg\"\n",
    "\n",
    "urban_centers_geoms = geopd.read_file(\n",
    "    ghs_geom_path,\n",
    "    layer='GHS_URBAN_SIMPLIFIED_fixed'    \n",
    ")\n",
    "urban_centers_geoms = urban_centers_geoms.rename(columns={\n",
    "    \"GC_UCN_MAI_2025\": \"name\",\n",
    "    \"GC_CNT_GAD_2025\": \"country\",\n",
    "})\n",
    "transform = pyproj.Transformer.from_crs(\"ESRI:54009\", \"EPSG:4326\", always_xy=True).transform\n",
    "\n",
    "urban_centers_info = geopd.read_file(\n",
    "    ghs_dataset_path,\n",
    "    layer='GHS_UCDB_THEME_GHSL_GLOBE_R2024A'    \n",
    ")\n",
    "urban_centers_info = urban_centers_info.rename(columns={\n",
    "    \"GC_POP_TOT_2025\": \"population\",\n",
    "    \"GC_UCA_KM2_2025\": \"area\",\n",
    "    \"GC_DEV_USR_2025\": \"continent\",\n",
    "    \"GC_UCN_MAI_2025\": \"name\",\n",
    "    \"GC_CNT_GAD_2025\": \"country\",\n",
    "    \"GC_DEV_WIG_2025\": \"income_group\",\n",
    "    \"GH_BUS_TOT_2025\": \"builtup_area\"\n",
    "})\n",
    "\n",
    "urban_centers_climate = geopd.read_file(\n",
    "    ghs_dataset_path,\n",
    "    layer='GHS_UCDB_THEME_CLIMATE_GLOBE_R2024A'    \n",
    ")\n",
    "urban_centers_climate = urban_centers_climate.rename(columns={\n",
    "    \"GC_UCN_MAI_2025\": \"name\",\n",
    "    \"GC_CNT_GAD_2025\": \"country\",\n",
    "    \"CL_KOP_CUR_2025\": \"climate_class\",\n",
    "})\n",
    "\n",
    "capital_cities = f\"{data_base_path}/capital_cities.csv\"\n",
    "capital_df = pd.read_csv(capital_cities, delimiter=\",\", header=None)\n",
    "countries = list(capital_df[0])\n",
    "city_names = list(capital_df[1])\n",
    "\n",
    "capitals = pd.DataFrame()\n",
    "for i in range(len(countries)):\n",
    "    capital_city = urban_centers_geoms.loc[\n",
    "        (urban_centers_geoms[\"name\"] == city_names[i]) &\n",
    "        (urban_centers_geoms[\"country\"] == countries[i])\n",
    "    ]\n",
    "    capitals = pd.concat([capitals, capital_city])\n",
    "\n",
    "capitals = capitals.reset_index(drop=True)\n",
    "\n",
    "\n",
    "capitals[\"display_name\"] = pd.Series()\n",
    "for cap in capitals.iterrows():\n",
    "    # the city name in lowercase and slug_case for creating the folder to store the graphs and shapefiles\n",
    "    city_name = cap[1][1].replace(\" \", \"_\").lower()\n",
    "    #city_name = \"buenos_aires\"\n",
    "\n",
    "    # The name to search the city in the GHS dataset. Capital case. Also used for the DEM.\n",
    "    search_name = cap[1][1]\n",
    "    #search_name = \"Buenos Aires\"\n",
    "\n",
    "    # The country in which the city is located for searching the GHS dataset. Capital case.\n",
    "    country = cap[1][3]\n",
    "    #country = \"Argentina\"\n",
    "\n",
    "    o_b = \"{\"\n",
    "    c_b = \"}\"\n",
    "    backslash = f\"\\\\\"\n",
    "    specials = \"áéíóú'șăŏã\"\n",
    "    replaces = \"aeiou_saoa\"\n",
    "\n",
    "    display_name = city_name\n",
    "    modified = False\n",
    "\n",
    "    if \"[\" in display_name or \"]\" in display_name:\n",
    "        modified  = True\n",
    "\n",
    "    for i in range(len(specials)):\n",
    "        if specials[i] in display_name:\n",
    "            modified  = True\n",
    "        display_name = display_name.replace(specials[i], replaces[i])\n",
    "        display_name = display_name.replace(\"[\", \"\")\n",
    "        display_name = display_name.replace(\"]\", \"\")\n",
    "\n",
    "    capitals.loc[cap[0], \"display_name\"] = display_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_mapping = {\n",
    "    \"Sub-Saharan Africa\": \"SA\",\n",
    "    \"Oceania\": \"OC\",\n",
    "    \"Northern America\": \"AM\",\n",
    "    \"Northern Africa and Western Asia\": \"NW\",\n",
    "    \"Latin America and the Caribbean\": \"LA\",\n",
    "    \"Europe\": \"EU\",\n",
    "    \"Eastern and South-Eastern Asia\": \"EA\",\n",
    "    \"Central and Southern Asia\": \"CA\",\n",
    "    \"Australia and New Zealand\": \"OC\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.indices import graph_based, proximity\n",
    "load_errors = []\n",
    "errors = []\n",
    "metrics_result_path = \"/home/user/Desktop/JP/street-network-indices/notebooks\"\n",
    "\n",
    "\n",
    "def add_result(results_dict, column, value):\n",
    "    print(f\"adding {column}\")\n",
    "    try:\n",
    "        results_dict[column].append(value)\n",
    "    except:\n",
    "        results_dict[column].append(None)\n",
    "\n",
    "\n",
    "for cap in capitals.iterrows():\n",
    "    display_name = cap[1][\"display_name\"]\n",
    "    city_name = display_name\n",
    "    # The name to search the city in the GHS dataset. Capital case. Also used for the DEM.\n",
    "    search_name = cap[1][1]\n",
    "\n",
    "    # The country in which the city is located for searching the GHS dataset. Capital case.\n",
    "    country = cap[1][3]\n",
    "    print(f\"starting {city_name}\")\n",
    "\n",
    "    # extract info from GHS with search_name and country\n",
    "    city_info = urban_centers_info.loc[\n",
    "        (urban_centers_info[\"name\"] == search_name) &\n",
    "        (urban_centers_info[\"country\"] == country)\n",
    "    ]\n",
    "\n",
    "    # extract climate info from GHS with search_name and country\n",
    "    city_climate = urban_centers_climate.loc[\n",
    "        (urban_centers_climate[\"name\"] == search_name) &\n",
    "        (urban_centers_climate[\"country\"] == country)\n",
    "    ]\n",
    "\n",
    "    # extract geometry from GHS with search_name and country\n",
    "    city_geom = urban_centers_geoms.loc[\n",
    "        (urban_centers_geoms[\"name\"] == search_name) &\n",
    "        (urban_centers_geoms[\"country\"] == country)\n",
    "    ]\n",
    "\n",
    "    results_dict = {\n",
    "        # general\n",
    "        \"city\": [],\n",
    "        \"area\": [],\n",
    "        \"geoarea\": [],\n",
    "        \"population\": [],\n",
    "        \"climate_class\": [], \n",
    "        \"builtup_area\": [],\n",
    "        \"income_group\": [],\n",
    "\n",
    "        \"orientation_entropy\": [],\n",
    "\n",
    "        # walking\n",
    "        \"walk_prox_poi_mean\": [],\n",
    "        \"walk_prox_poi_median\": [],\n",
    "        \"walk_prox_poi_range\": [],\n",
    "        \"walk_prox_poi_std\": [],\n",
    "        \"walk_prox_poi_iqr\": [],\n",
    "\n",
    "        \"walk_prox_pub_transport_mean\": [],\n",
    "        \"walk_prox_pub_transport_median\": [],\n",
    "        \"walk_prox_pub_transport_range\": [],\n",
    "        \"walk_prox_pub_transport_std\": [],\n",
    "        \"walk_prox_pub_transport_iqr\": [],\n",
    "\n",
    "        \"walk_circuity\": [],\n",
    "\n",
    "        \"walk_street_len_mean\": [],\n",
    "        \"walk_street_len_median\": [],\n",
    "        \"walk_street_len_range\": [],\n",
    "        \"walk_street_len_std\": [],\n",
    "        \"walk_street_len_iqr\": [],\n",
    "        \"walk_street_density\": [],\n",
    "        \"walk_street_len_total\": [],\n",
    "\n",
    "        \"walk_link_node_ratio\": [],\n",
    "\n",
    "        \"walk_intersection_density\": [],\n",
    "        \"walk_intersection_count\": [],\n",
    "\n",
    "        # cycling\n",
    "        \"bike_prox_poi_mean\": [],\n",
    "        \"bike_prox_poi_median\": [],\n",
    "        \"bike_prox_poi_range\": [],\n",
    "        \"bike_prox_poi_std\": [],\n",
    "        \"bike_prox_poi_iqr\": [],\n",
    "\n",
    "        \"bike_slope_mean\": [],\n",
    "        \"bike_slope_median\": [],\n",
    "        \"bike_slope_range\": [],\n",
    "        \"bike_slope_std\": [],\n",
    "        \"bike_slope_iqr\": [],\n",
    "\n",
    "        \"bike_circuity\": [],\n",
    "        \n",
    "        \"bike_street_len_mean\": [],\n",
    "        \"bike_street_len_median\": [],\n",
    "        \"bike_street_len_range\": [],\n",
    "        \"bike_street_len_std\": [],\n",
    "        \"bike_street_len_iqr\": [],\n",
    "        \"bike_street_density\": [],\n",
    "        \"bike_street_len_total\": [],\n",
    "\n",
    "        \"bike_link_node_ratio\": [],\n",
    "\n",
    "        \"bike_intersection_density\": [],\n",
    "        \"bike_intersection_count\": [],\n",
    "\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        Path(f\"{data_base_path}/{city_name}/indices\").mkdir(parents=True, exist_ok=True) #\n",
    "\n",
    "        full_name = capitals.loc[capitals[\"display_name\"] == city_name]\n",
    "        full_name = full_name[\"name\"].values[0]\n",
    "\n",
    "        area = city_info[\"area\"]\n",
    "        area = area.values[0]\n",
    "\n",
    "        population = city_info[\"population\"]\n",
    "        population = population.values[0]\n",
    "\n",
    "        builtup_area = city_info[\"builtup_area\"] # originally in m2\n",
    "        builtup_area = builtup_area.values[0] * (10 ** -6) # convert to km2\n",
    "\n",
    "        geoarea = city_info[\"continent\"]\n",
    "        geoarea = continent_mapping[geoarea.values[0]]\n",
    "\n",
    "        income_group = city_info[\"income_group\"]\n",
    "        income_group = income_group.values[0]\n",
    "\n",
    "        climate_class = city_climate[\"climate_class\"]\n",
    "        climate_class = climate_class.values[0]\n",
    "\n",
    "        geometry_urban_centers = city_geom[\"geometry\"]\n",
    "        geom = geometry_urban_centers.values[0]\n",
    "        geom = ops.transform(transform, geom)\n",
    "\n",
    "        add_result(results_dict, \"city\", city_name)\n",
    "        add_result(results_dict, \"area\", area)\n",
    "        add_result(results_dict, \"geoarea\", geoarea)\n",
    "        add_result(results_dict, \"population\", population)\n",
    "        add_result(results_dict, \"builtup_area\", builtup_area)\n",
    "        add_result(results_dict, \"income_group\", income_group)\n",
    "        add_result(results_dict, \"climate_class\", climate_class)\n",
    "\n",
    "        # Indicators using pedestrian network\n",
    "        try:\n",
    "            g_walk = extractor.load_graph(f'{city_name}/graph/walk_{city_name}')\n",
    "            g_public = extractor.load_graph(f'{city_name}/graph/public_{city_name}')\n",
    "        except Exception as ex:\n",
    "            load_errors.append(city_name)\n",
    "            continue\n",
    "        \n",
    "        # walking proximity to pois\n",
    "        walk_prox_poi_mean, walk_prox_poi_median, walk_prox_poi_range,  walk_prox_poi_std, walk_prox_poi_iqr = proximity.proximity_to_pois(g_walk, geom, walk_time=15, parallelize=True, max_partition_size=5000)\n",
    "        add_result(results_dict, \"walk_prox_poi_mean\", walk_prox_poi_mean)\n",
    "        add_result(results_dict, \"walk_prox_poi_median\", walk_prox_poi_median)\n",
    "        add_result(results_dict, \"walk_prox_poi_range\", walk_prox_poi_range)\n",
    "        add_result(results_dict, \"walk_prox_poi_std\", walk_prox_poi_std)\n",
    "        add_result(results_dict, \"walk_prox_poi_iqr\", walk_prox_poi_iqr)\n",
    "\n",
    "        # walking proximity to public transport\n",
    "        walk_prox_pub_transport_mean, walk_prox_pub_transport_median, walk_prox_pub_transport_range, walk_prox_pub_transport_std, walk_prox_pub_transport_iqr = proximity.proximity_to_public_transport(g_walk, g_public, distance=400, parallelize=True, max_partition_size=5000)\n",
    "        add_result(results_dict, \"walk_prox_pub_transport_mean\", walk_prox_pub_transport_mean)\n",
    "        add_result(results_dict, \"walk_prox_pub_transport_median\", walk_prox_pub_transport_median)\n",
    "        add_result(results_dict, \"walk_prox_pub_transport_range\", walk_prox_pub_transport_range)\n",
    "        add_result(results_dict, \"walk_prox_pub_transport_std\", walk_prox_pub_transport_std)\n",
    "        add_result(results_dict, \"walk_prox_pub_transport_iqr\", walk_prox_pub_transport_iqr)\n",
    "\n",
    "        # Walking Circuity\n",
    "        walk_circuity = graph_based.circuity(g_walk, add_property=True)\n",
    "        add_result(results_dict, \"walk_circuity\", walk_circuity)\n",
    "\n",
    "        # Walking street length\n",
    "        walk_st_len_mean, walk_st_len_med, walk_st_len_range, walk_st_len_std, walk_st_len_iqr, walk_st_len_total = graph_based.street_length(g_walk)\n",
    "        add_result(results_dict, \"walk_street_len_mean\", walk_st_len_mean)\n",
    "        add_result(results_dict, \"walk_street_len_median\", walk_st_len_med)\n",
    "        add_result(results_dict, \"walk_street_len_range\", walk_st_len_range)\n",
    "        add_result(results_dict, \"walk_street_len_std\", walk_st_len_std)\n",
    "        add_result(results_dict, \"walk_street_len_iqr\", walk_st_len_iqr)\n",
    "        add_result(results_dict, \"walk_street_density\", walk_st_len_total / area)\n",
    "        add_result(results_dict, \"walk_street_len_total\", walk_st_len_total)\n",
    "\n",
    "        # Walking Link-Node ratio\n",
    "        walk_link_node_ratio = graph_based.link_node_ratio(g_walk)\n",
    "        add_result(results_dict, \"walk_link_node_ratio\", walk_link_node_ratio)\n",
    "\n",
    "        # Walking intersection density\n",
    "        walk_intersection_density, walk_intersection_count = graph_based.intersection_density(g_walk, area)\n",
    "        add_result(results_dict, \"walk_intersection_density\", walk_intersection_density)\n",
    "        add_result(results_dict, \"walk_intersection_count\", walk_intersection_count)\n",
    "\n",
    "        # save the pedestrian graph with indicators attached\n",
    "        extractor.save_as_shp(g_walk, f'{city_name}/indices/walk_indices_{city_name}')\n",
    "        # free memory\n",
    "        del g_walk\n",
    "        del g_public\n",
    "\n",
    "        # Indicators using the cycling network\n",
    "        try:\n",
    "            g_bike = extractor.load_graph(f'{city_name}/graph/bike_{city_name}')\n",
    "        except Exception as ex:\n",
    "            load_errors.append(city_name)\n",
    "            continue\n",
    "\n",
    "        # cycling proximity to POIs\n",
    "        bike_prox_poi_mean, bike_prox_poi_median, bike_prox_poi_range, bike_prox_poi_std, bike_prox_poi_iqr = proximity.proximity_to_pois(g_bike, geom, walk_time=15, parallelize=True, max_partition_size=5000)\n",
    "        add_result(results_dict, \"bike_prox_poi_mean\", bike_prox_poi_mean)\n",
    "        add_result(results_dict, \"bike_prox_poi_median\", bike_prox_poi_median)\n",
    "        add_result(results_dict, \"bike_prox_poi_range\", bike_prox_poi_range)\n",
    "        add_result(results_dict, \"bike_prox_poi_std\", bike_prox_poi_std)\n",
    "        add_result(results_dict, \"bike_prox_poi_iqr\", bike_prox_poi_iqr)\n",
    "\n",
    "        # Cycling slope\n",
    "        bike_slope_mean, bike_slope_median, bike_slope_range, bike_slope_std, bike_slope_iqr = graph_based.steepness(g_bike)\n",
    "        add_result(results_dict, \"bike_slope_mean\", bike_slope_mean)\n",
    "        add_result(results_dict, \"bike_slope_median\", bike_slope_median)\n",
    "        add_result(results_dict, \"bike_slope_range\", bike_slope_range)\n",
    "        add_result(results_dict, \"bike_slope_std\", bike_slope_std)\n",
    "        add_result(results_dict, \"bike_slope_iqr\", bike_slope_iqr)\n",
    "\n",
    "        # Cycling circuity\n",
    "        bike_circuity = graph_based.circuity(g_bike, add_property=True)\n",
    "        add_result(results_dict, \"bike_circuity\", bike_circuity)\n",
    "\n",
    "        # Cycling street length\n",
    "        bike_st_len_mean, bike_st_len_med, bike_st_len_range, bike_st_len_std, bike_st_len_iqr, bike_st_len_total = graph_based.street_length(g_bike)\n",
    "        add_result(results_dict, \"bike_street_len_mean\", bike_st_len_mean)\n",
    "        add_result(results_dict, \"bike_street_len_median\", bike_st_len_med)\n",
    "        add_result(results_dict, \"bike_street_len_range\", bike_st_len_range)\n",
    "        add_result(results_dict, \"bike_street_len_std\", bike_st_len_std)\n",
    "        add_result(results_dict, \"bike_street_len_iqr\", bike_st_len_iqr)\n",
    "        add_result(results_dict, \"bike_street_density\", bike_st_len_total / area)\n",
    "        add_result(results_dict, \"bike_street_len_total\", bike_st_len_total)\n",
    "\n",
    "        # Cycling Link-Node ratio\n",
    "        bike_link_node_ratio = graph_based.link_node_ratio(g_bike)\n",
    "        add_result(results_dict, \"bike_link_node_ratio\", bike_link_node_ratio)\n",
    "\n",
    "        # Cycling intersection density\n",
    "        bike_intersection_density, bike_intersection_count = graph_based.intersection_density(g_bike, area)\n",
    "        add_result(results_dict, \"bike_intersection_density\", bike_intersection_density)\n",
    "        add_result(results_dict, \"bike_intersection_count\", bike_intersection_count)\n",
    "\n",
    "        # save the cycling graph with indicators attached\n",
    "        extractor.save_as_shp(g_bike, f'{city_name}/indices/bike_indices_{city_name}')\n",
    "        # free memory\n",
    "        del g_bike\n",
    "\n",
    "        # Indicators using the driving network\n",
    "        try:\n",
    "            g_drive = extractor.load_graph(f'{city_name}/graph/drive_{city_name}')\n",
    "        except Exception as ex:\n",
    "            load_errors.append(city_name)\n",
    "            continue\n",
    "\n",
    "        orientation_entropy = graph_based.orientation_entropy(g_drive)\n",
    "        add_result(results_dict, \"orientation_entropy\", orientation_entropy)\n",
    "\n",
    "        # free memory\n",
    "        del g_drive\n",
    "\n",
    "        results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "        try:\n",
    "            results_df_old = pd.read_csv(f'{metrics_result_path}/results_temp.csv', index_col=0)\n",
    "            results_df_new = pd.concat([results_df_old, results_df]).reset_index(drop=True)\n",
    "            results_df_new.to_csv(f'{metrics_result_path}/results_temp.csv')\n",
    "        except:\n",
    "            print(\"First time\")\n",
    "            results_df.to_csv(f'{metrics_result_path}/results_temp.csv')\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "        print(ex.with_traceback())\n",
    "        print(city_name)\n",
    "        errors.append(city_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the cities with errors, if any\n",
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
